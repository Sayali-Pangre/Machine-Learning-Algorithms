#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Mar 13 14:01:39 2019

@author: dell
"""

import numpy 
import matplotlib.pyplot as plt
import theano 

x=theano.tensor.matrix(name='x')
#define the theano variables,inputs,weights and bias
xdata=[[0,0],[0,1],[1,0],[0,0]]
ydata=[0,1,1,0]

#hidden layer(layer 1)
w1val=numpy.asarray([numpy.random.randn(),numpy.random.randn()])
w2val=numpy.asarray([numpy.random.randn(),numpy.random.randn()])
w3val=numpy.asarray([numpy.random.randn(),numpy.random.randn()])

w1=theano.shared(w1val,name='w1')
w2=theano.shared(w2val,name='w2')
w3=theano.shared(w3val,name='w3')

#bias value is 1
b1=theano.shared(1.1,name='b1')
b2=theano.shared(1.2,name='b2')
b3=theano.shared(1.3,name='b3')

#computation for every node
a1sum=theano.tensor.dot(x,w1)+b1
a2sum=theano.tensor.dot(x,w2)+b2

a1hat=1/(1+theano.tensor.exp(-a1sum))
a2hat=1/(1+theano.tensor.exp(-a2sum))

#output layer neuron
#stack is cobining two hiding layer values and feeding to the output layer

x2=theano.tensor.stack([a1hat,a2hat],axis=1)

a3sum=theano.tensor.dot(x2,w3)+b3

ahat=1/(1+theano.tensor.exp(-a3sum))   #ahat is predicted y
#defining the correct output variable
a=theano.tensor.vector('a')

#-(ylog(yhat)+(1-y)log(1-yhat))
cost=-(a*theano.tensor.log(ahat)+(1-a)*theano.tensor.log(1-ahat)).sum()

#gradientDescent
dcostdw1=theano.tensor.grad(cost,w1)
dcostdw2=theano.tensor.grad(cost,w2)
dcostdw3=theano.tensor.grad(cost,w3)

dcostdb1=theano.tensor.grad(cost,b1)
dcostdb2=theano.tensor.grad(cost,b2)
dcostdb3=theano.tensor.grad(cost,b3)

#apply GDA to compute the updated weight
w1new=w1-0.005*dcostdw1
w2new=w2-0.005*dcostdw2
w3new=w3-0.005*dcostdw3

b1new=b1-0.005*dcostdb1
b2new=b2-0.005*dcostdb2
b3new=b3-0.005*dcostdb3

#training function
train=theano.function([x,a],[ahat,cost],updates=[(w1,w1new),(w2,w2new),(w3,w3new),(b1,b1new),(b2,b2new),(b3,b3new)])

cost1=[]
for i in range(80000):
    pval,costval=train(xdata,ydata)
    print(costval)
    cost1.append(costval)
    
print('The final outputs are:')
for i in range(len(xdata)):
    print("The o/p of x1=%d and x2=%d is %.3f"%(xdata[i][0],xdata[i][1],pval[i]))

plt.plot(cost1,color='red')
plt.show()

#activation function is for every node
